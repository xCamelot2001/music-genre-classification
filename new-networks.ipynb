{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d0a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def custom_gray_loader(path: str):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('L')  # Grayscale\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32):\n",
    "    # === Step 1: Temporary transform for stats ===\n",
    "    temp_transform = transforms.Compose([\n",
    "        transforms.Resize((180, 180)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    dataset_temp = datasets.ImageFolder(root=data_dir, transform=temp_transform, loader=custom_gray_loader)\n",
    "    \n",
    "    # === Step 2: Split the dataset ===\n",
    "    total_length = len(dataset_temp)\n",
    "    train_len = int(0.7 * total_length)\n",
    "    val_len = int(0.2 * total_length)\n",
    "    test_len = total_length - train_len - val_len\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset_temp, [train_len, val_len, test_len],\n",
    "                                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # === Step 3: Compute mean & std on training set ===\n",
    "    loader_for_stats = DataLoader(train_set, batch_size=batch_size)\n",
    "    mean, std, total_pixels = 0.0, 0.0, 0\n",
    "\n",
    "    for images, _ in loader_for_stats:\n",
    "        mean += images.sum().item()\n",
    "        std += (images ** 2).sum().item()\n",
    "        total_pixels += images.numel()\n",
    "\n",
    "    mean /= total_pixels\n",
    "    std = (std / total_pixels - mean ** 2) ** 0.5\n",
    "\n",
    "    print(f\"ðŸ“Š Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "\n",
    "    # === Step 4: Final transforms â€” gentle, FCN-safe, CNN-compatible ===\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((180, 180)),\n",
    "        transforms.RandomRotation(degrees=10),                      # Safe small rotation\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0), fill=0)\n",
    "        ], p=0.1),                                                  # Tiny shifts\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    transform_val_test = transforms.Compose([\n",
    "        transforms.Resize((180, 180)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    # === Step 5: Reload dataset with new transforms ===\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir, loader=custom_gray_loader)\n",
    "\n",
    "    full_dataset.transform = transform_train\n",
    "    train_set.dataset = full_dataset\n",
    "\n",
    "    full_dataset.transform = transform_val_test\n",
    "    val_set.dataset = full_dataset\n",
    "    test_set.dataset = full_dataset\n",
    "\n",
    "    # === Step 6: Create dataloaders ===\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c662fc",
   "metadata": {},
   "source": [
    "# Net1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502351f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(180 * 180, 512)      # Hidden layer 1\n",
    "        self.fc2 = nn.Linear(512, 128)            # Hidden layer 2\n",
    "        self.out = nn.Linear(128, 10)             # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)     # Flatten input\n",
    "        x = self.relu(self.fc1(x))   # Hidden layer 1 + activation\n",
    "        x = self.relu(self.fc2(x))   # Hidden layer 2 + activation\n",
    "        return self.out(x)           # Output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2b99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=50):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "        print(f\"ðŸ“… Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {val_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e71de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Mean: 0.4673, Std: 0.4524\n",
      "ðŸ“… Epoch 1/80 | Loss: 52.7691 | Val Acc: 18.09%\n",
      "ðŸ“… Epoch 2/80 | Loss: 49.3363 | Val Acc: 18.09%\n",
      "ðŸ“… Epoch 3/80 | Loss: 46.9974 | Val Acc: 16.58%\n",
      "ðŸ“… Epoch 4/80 | Loss: 45.8357 | Val Acc: 22.11%\n",
      "ðŸ“… Epoch 5/80 | Loss: 44.3001 | Val Acc: 23.62%\n",
      "ðŸ“… Epoch 6/80 | Loss: 43.3984 | Val Acc: 28.64%\n",
      "ðŸ“… Epoch 7/80 | Loss: 43.0898 | Val Acc: 26.63%\n",
      "ðŸ“… Epoch 8/80 | Loss: 41.5627 | Val Acc: 23.12%\n",
      "ðŸ“… Epoch 9/80 | Loss: 41.4709 | Val Acc: 30.65%\n",
      "ðŸ“… Epoch 10/80 | Loss: 40.3104 | Val Acc: 27.14%\n",
      "ðŸ“… Epoch 11/80 | Loss: 40.7025 | Val Acc: 28.64%\n",
      "ðŸ“… Epoch 12/80 | Loss: 40.0097 | Val Acc: 27.14%\n",
      "ðŸ“… Epoch 13/80 | Loss: 38.8195 | Val Acc: 26.13%\n",
      "ðŸ“… Epoch 14/80 | Loss: 38.5824 | Val Acc: 29.65%\n",
      "ðŸ“… Epoch 15/80 | Loss: 38.6603 | Val Acc: 33.17%\n",
      "ðŸ“… Epoch 16/80 | Loss: 37.7951 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 17/80 | Loss: 37.3837 | Val Acc: 33.17%\n",
      "ðŸ“… Epoch 18/80 | Loss: 37.5153 | Val Acc: 32.16%\n",
      "ðŸ“… Epoch 19/80 | Loss: 37.2706 | Val Acc: 26.13%\n",
      "ðŸ“… Epoch 20/80 | Loss: 36.9550 | Val Acc: 30.15%\n",
      "ðŸ“… Epoch 21/80 | Loss: 35.6754 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 22/80 | Loss: 35.4742 | Val Acc: 35.68%\n",
      "ðŸ“… Epoch 23/80 | Loss: 35.8856 | Val Acc: 34.67%\n",
      "ðŸ“… Epoch 24/80 | Loss: 35.3844 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 25/80 | Loss: 35.0998 | Val Acc: 31.16%\n",
      "ðŸ“… Epoch 26/80 | Loss: 35.4077 | Val Acc: 37.69%\n",
      "ðŸ“… Epoch 27/80 | Loss: 35.2405 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 28/80 | Loss: 34.2934 | Val Acc: 28.64%\n",
      "ðŸ“… Epoch 29/80 | Loss: 33.5719 | Val Acc: 30.65%\n",
      "ðŸ“… Epoch 30/80 | Loss: 33.7487 | Val Acc: 35.68%\n",
      "ðŸ“… Epoch 31/80 | Loss: 32.4682 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 32/80 | Loss: 33.9962 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 33/80 | Loss: 33.4447 | Val Acc: 38.69%\n",
      "ðŸ“… Epoch 34/80 | Loss: 32.4841 | Val Acc: 30.15%\n",
      "ðŸ“… Epoch 35/80 | Loss: 31.9371 | Val Acc: 35.68%\n",
      "ðŸ“… Epoch 36/80 | Loss: 31.5926 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 37/80 | Loss: 31.7314 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 38/80 | Loss: 31.0351 | Val Acc: 35.18%\n",
      "ðŸ“… Epoch 39/80 | Loss: 30.6655 | Val Acc: 40.70%\n",
      "ðŸ“… Epoch 40/80 | Loss: 30.1562 | Val Acc: 37.69%\n",
      "ðŸ“… Epoch 41/80 | Loss: 30.5621 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 42/80 | Loss: 29.8866 | Val Acc: 38.69%\n",
      "ðŸ“… Epoch 43/80 | Loss: 30.8979 | Val Acc: 34.67%\n",
      "ðŸ“… Epoch 44/80 | Loss: 30.1795 | Val Acc: 42.21%\n",
      "ðŸ“… Epoch 45/80 | Loss: 30.1474 | Val Acc: 37.69%\n",
      "ðŸ“… Epoch 46/80 | Loss: 29.2772 | Val Acc: 35.68%\n",
      "ðŸ“… Epoch 47/80 | Loss: 28.8461 | Val Acc: 33.67%\n",
      "ðŸ“… Epoch 48/80 | Loss: 28.9547 | Val Acc: 34.67%\n",
      "ðŸ“… Epoch 49/80 | Loss: 29.0343 | Val Acc: 34.67%\n",
      "ðŸ“… Epoch 50/80 | Loss: 28.4119 | Val Acc: 40.70%\n",
      "ðŸ“… Epoch 51/80 | Loss: 27.3000 | Val Acc: 38.19%\n",
      "ðŸ“… Epoch 52/80 | Loss: 27.7697 | Val Acc: 39.70%\n",
      "ðŸ“… Epoch 53/80 | Loss: 27.7377 | Val Acc: 35.68%\n",
      "ðŸ“… Epoch 54/80 | Loss: 26.8975 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 55/80 | Loss: 27.1159 | Val Acc: 39.20%\n",
      "ðŸ“… Epoch 56/80 | Loss: 26.7135 | Val Acc: 36.18%\n",
      "ðŸ“… Epoch 57/80 | Loss: 26.0741 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 58/80 | Loss: 25.9467 | Val Acc: 37.19%\n",
      "ðŸ“… Epoch 59/80 | Loss: 26.3996 | Val Acc: 36.68%\n",
      "ðŸ“… Epoch 60/80 | Loss: 25.9811 | Val Acc: 32.16%\n",
      "ðŸ“… Epoch 61/80 | Loss: 25.2934 | Val Acc: 34.17%\n",
      "ðŸ“… Epoch 62/80 | Loss: 26.4860 | Val Acc: 32.66%\n",
      "ðŸ“… Epoch 63/80 | Loss: 25.3483 | Val Acc: 36.68%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)  \u001b[38;5;66;03m# ðŸ”¥ Lower LR!\u001b[39;00m\n\u001b[32m      8\u001b[39m loss_fn = nn.CrossEntropyLoss()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_accuracy(model,\u001b[38;5;250m \u001b[39mtrain_loader,\u001b[38;5;250m \u001b[39mdevice)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluate_accuracy(model,\u001b[38;5;250m \u001b[39mval_loader,\u001b[38;5;250m \u001b[39mdevice)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, loss_fn, device, epochs)\u001b[39m\n\u001b[32m     24\u001b[39m     optimizer.step()\n\u001b[32m     25\u001b[39m     total_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m val_acc = \u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“… Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mevaluate_accuracy\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n\u001b[32m      3\u001b[39m correct, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    index (int): Index\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m \u001b[33;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    244\u001b[39m path, target = \u001b[38;5;28mself\u001b[39m.samples[index]\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    247\u001b[39m     sample = \u001b[38;5;28mself\u001b[39m.transform(sample)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mcustom_gray_loader\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      9\u001b[39m     img = Image.open(f)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\PIL\\Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\DLT-1st-Cw\\music-classification\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = \"./Data/images_original\"\n",
    "train_loader, val_loader, test_loader, mean, std = get_dataloaders(data_path, batch_size=32)\n",
    "\n",
    "model = Net1().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # ðŸ”¥ Lower LR!\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=50)\n",
    "\n",
    "# I want to save the model after training\n",
    "torch.save(model.state_dict(), 'Net1-50epoch.pth')\n",
    "\n",
    "print(f\"âœ… Train Acc: {evaluate_accuracy(model, train_loader, device):.2%}\")\n",
    "print(f\"âœ… Val Acc: {evaluate_accuracy(model, val_loader, device):.2%}\")\n",
    "print(f\"âœ… Test Acc: {evaluate_accuracy(model, test_loader, device):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = \"./Data/images_original\"\n",
    "train_loader, val_loader, test_loader, mean, std = get_dataloaders(data_path, batch_size=32)\n",
    "\n",
    "model = Net1().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # ðŸ”¥ Lower LR!\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=100)\n",
    "\n",
    "# I want to save the model after training\n",
    "torch.save(model.state_dict(), 'Net1-100epoch.pth')\n",
    "\n",
    "print(f\"âœ… Train Acc: {evaluate_accuracy(model, train_loader, device):.2%}\")\n",
    "print(f\"âœ… Val Acc: {evaluate_accuracy(model, val_loader, device):.2%}\")\n",
    "print(f\"âœ… Test Acc: {evaluate_accuracy(model, test_loader, device):.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
