{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc877eb",
   "metadata": {},
   "source": [
    "# Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Genre Classification using Deep Learning\n",
    "# COMP6252 Coursework 1\n",
    "\n",
    "## Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "from PIL import Image\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, TensorDataset, ConcatDataset\n",
    "import torchvision.datasets as datasets\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "## Global Setup\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Global constants\n",
    "NUM_CLASSES = 10  # 10 music genres\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE_DEFAULT = 0.00001\n",
    "IMAGE_SIZE = 180  # 180x180 for spectrograms as required by the coursework\n",
    "\n",
    "# Setup GPU Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1bfc0",
   "metadata": {},
   "source": [
    "# Spectrogram Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spectrogram Generation Functions\n",
    "\n",
    "def check_if_spectrograms_exist(wav_path, output_path, tolerance=1):\n",
    "    \"\"\"\n",
    "    Check if spectrograms already exist for audio files, with tolerance for a few missing files.\n",
    "    \n",
    "    Args:\n",
    "        wav_path (str): Path to the directory containing WAV files organized by genre\n",
    "        output_path (str): Path where spectrogram images should be saved\n",
    "        tolerance (int): Number of files that can be missing per genre (default=1)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if spectrograms exist (within tolerance), False otherwise\n",
    "    \"\"\"\n",
    "    # If output directory doesn't exist, spectrograms don't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        return False\n",
    "    \n",
    "    # Get list of genres from audio files\n",
    "    audio_genres = [g for g in os.listdir(wav_path) if os.path.isdir(os.path.join(wav_path, g))]\n",
    "    \n",
    "    # Track total files and mismatches\n",
    "    total_wav_files = 0\n",
    "    total_png_files = 0\n",
    "    total_mismatches = 0\n",
    "    \n",
    "    # Check if each genre folder exists in the output directory and has similar number of files\n",
    "    for genre in audio_genres:\n",
    "        audio_genre_path = os.path.join(wav_path, genre)\n",
    "        spec_genre_path = os.path.join(output_path, genre)\n",
    "        \n",
    "        # Check if genre folder exists in output directory\n",
    "        if not os.path.exists(spec_genre_path):\n",
    "            return False\n",
    "            \n",
    "        # Count number of wav files and png files\n",
    "        wav_files = [f for f in os.listdir(audio_genre_path) if f.endswith('.wav')]\n",
    "        png_files = [f for f in os.listdir(spec_genre_path) if f.endswith('.png')]\n",
    "        \n",
    "        wav_count = len(wav_files)\n",
    "        png_count = len(png_files)\n",
    "        \n",
    "        # Check for mismatches\n",
    "        difference = wav_count - png_count\n",
    "        \n",
    "        # If the difference is more than the tolerance, regenerate\n",
    "        if difference > tolerance:\n",
    "            print(f\"Genre {genre}: {wav_count} WAV files but only {png_count} PNG files\")\n",
    "            return False\n",
    "        \n",
    "        # If we have more PNGs than WAVs, something is wrong\n",
    "        if difference < 0:\n",
    "            print(f\"Genre {genre}: More PNG files ({png_count}) than WAV files ({wav_count})! Might need cleaning.\")\n",
    "        \n",
    "        total_wav_files += wav_count\n",
    "        total_png_files += png_count\n",
    "        total_mismatches += max(0, difference)  # Only count missing files, not excess\n",
    "    \n",
    "    # Check if total mismatches exceed global tolerance\n",
    "    if total_mismatches > tolerance * len(audio_genres):\n",
    "        print(f\"Total missing spectrograms: {total_mismatches} (tolerance: {tolerance * len(audio_genres)})\")\n",
    "        return False\n",
    "    \n",
    "    # Check for exact file correspondence, not just counts\n",
    "    for genre in audio_genres:\n",
    "        audio_genre_path = os.path.join(wav_path, genre)\n",
    "        spec_genre_path = os.path.join(output_path, genre)\n",
    "        \n",
    "        wav_files = [os.path.splitext(f)[0] for f in os.listdir(audio_genre_path) if f.endswith('.wav')]\n",
    "        png_files = [os.path.splitext(f)[0] for f in os.listdir(spec_genre_path) if f.endswith('.png')]\n",
    "        \n",
    "        # Check if most wav files have corresponding png files\n",
    "        files_without_spectrograms = [f for f in wav_files if f not in png_files]\n",
    "        if len(files_without_spectrograms) > tolerance:\n",
    "            print(f\"Genre {genre} has {len(files_without_spectrograms)} WAV files without spectrograms\")\n",
    "            return False\n",
    "    \n",
    "    print(f\"Spectrograms exist: {total_png_files} spectrograms for {total_wav_files} WAV files\")\n",
    "    return True\n",
    "\n",
    "def create_mel_spectrogram(file_path, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Create mel spectrogram from an audio file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file\n",
    "        n_fft (int): Length of the FFT window\n",
    "        hop_length (int): Number of samples between successive frames\n",
    "        n_mels (int): Number of Mel bands\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (S_dB, sr) - log-scaled mel spectrogram and sample rate\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Generate mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, \n",
    "        sr=sr, \n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length, \n",
    "        n_mels=n_mels)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    return S_dB, sr\n",
    "\n",
    "def save_spectrogram_as_image(S_dB, sr, hop_length, output_file_path, image_size=180):\n",
    "    \"\"\"\n",
    "    Save a spectrogram as a grayscale image.\n",
    "    \n",
    "    Args:\n",
    "        S_dB (numpy.ndarray): Log-scaled mel spectrogram\n",
    "        sr (int): Sample rate\n",
    "        hop_length (int): Number of samples between successive frames\n",
    "        output_file_path (str): Path where to save the image\n",
    "        image_size (int): Size of output image (square)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the image was saved successfully\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Plot spectrogram\n",
    "        plt.figure(figsize=(3, 3), dpi=60)\n",
    "        librosa.display.specshow(S_dB, sr=sr, hop_length=hop_length, cmap='gray_r')\n",
    "        plt.axis('off')  # Remove axis\n",
    "        \n",
    "        # Save image into buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        \n",
    "        # Load from buffer, resize, and save\n",
    "        buf.seek(0)\n",
    "        image = Image.open(buf).convert('L')  # Convert to grayscale ('L' mode)\n",
    "        image = image.resize((image_size, image_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Save final image\n",
    "        image.save(output_file_path)\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving spectrogram to {output_file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_genre_folder(genre_path, output_genre_path, n_fft=2048, hop_length=512, n_mels=128, image_size=180):\n",
    "    \"\"\"\n",
    "    Process all audio files in a genre folder and create spectrograms.\n",
    "    \n",
    "    Args:\n",
    "        genre_path (str): Path to the genre folder containing WAV files\n",
    "        output_genre_path (str): Path where spectrogram images will be saved\n",
    "        n_fft (int): Length of the FFT window\n",
    "        hop_length (int): Number of samples between successive frames\n",
    "        n_mels (int): Number of Mel bands\n",
    "        image_size (int): Size of output images\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_count, error_count) - number of files processed and errors\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_genre_path, exist_ok=True)\n",
    "    \n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for file in os.listdir(genre_path):\n",
    "        if not file.endswith('.wav'):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(genre_path, file)\n",
    "        image_filename = os.path.splitext(file)[0] + '.png'\n",
    "        output_file_path = os.path.join(output_genre_path, image_filename)\n",
    "        \n",
    "        # Skip if this specific file already exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Create mel spectrogram\n",
    "            S_dB, sr = create_mel_spectrogram(file_path, n_fft, hop_length, n_mels)\n",
    "            \n",
    "            # Save spectrogram as image\n",
    "            if save_spectrogram_as_image(S_dB, sr, hop_length, output_file_path, image_size):\n",
    "                processed_count += 1\n",
    "                print(f\"Saved spectrogram for {file_path} as {image_filename}\")\n",
    "            else:\n",
    "                error_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            error_count += 1\n",
    "    \n",
    "    return processed_count, error_count\n",
    "\n",
    "def generate_spectrograms(wav_path, output_path, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Generate mel-spectrograms from audio files and save them as grayscale images.\n",
    "    \n",
    "    Args:\n",
    "        wav_path (str): Path to the directory containing WAV files organized by genre\n",
    "        output_path (str): Path where spectrogram images will be saved\n",
    "        n_fft (int): Length of the FFT window\n",
    "        hop_length (int): Number of samples between successive frames\n",
    "        n_mels (int): Number of Mel bands\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if spectrograms were generated, False if they already existed\n",
    "    \"\"\"\n",
    "    # Check if spectrograms already exist\n",
    "    if check_if_spectrograms_exist(wav_path, output_path):\n",
    "        print(\"Spectrogram images already exist. Skipping generation.\")\n",
    "        return False\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    print(\"Generating mel-spectrograms...\")\n",
    "    genres = os.listdir(wav_path)\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    for genre in genres:\n",
    "        genre_path = os.path.join(wav_path, genre)\n",
    "        \n",
    "        if not os.path.isdir(genre_path):\n",
    "            continue\n",
    "        \n",
    "        output_genre_path = os.path.join(output_path, genre)\n",
    "        \n",
    "        # Process all files in the genre folder\n",
    "        processed, errors = process_genre_folder(\n",
    "            genre_path, \n",
    "            output_genre_path, \n",
    "            n_fft, \n",
    "            hop_length, \n",
    "            n_mels, \n",
    "            IMAGE_SIZE\n",
    "        )\n",
    "        \n",
    "        total_processed += processed\n",
    "        total_errors += errors\n",
    "    \n",
    "    print(f\"Spectrogram generation complete. Generated {total_processed} spectrograms with {total_errors} errors.\")\n",
    "    return True\n",
    "\n",
    "# Define paths for dataset\n",
    "wav_dataset_path = 'Data/genres_original'\n",
    "spectrogram_path = 'Data/greyscale_spectrograms'\n",
    "\n",
    "# Generate spectrograms if needed\n",
    "spectrograms_generated = generate_spectrograms(\n",
    "    wav_path=wav_dataset_path, \n",
    "    output_path=spectrogram_path,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84348c23",
   "metadata": {},
   "source": [
    "# Data Loading and Processing for CNN Models (1-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading and Processing for CNN Models (1-4)\n",
    "\n",
    "def custom_gray_loader(path: str):\n",
    "    \"\"\"Load image as grayscale\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('L')  # Grayscale\n",
    "\n",
    "def get_dataloaders(data_dir, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders with appropriate transformations.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing the dataset organized by class\n",
    "        batch_size (int): Batch size for dataloaders\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader, mean, std)\n",
    "    \"\"\"\n",
    "    # === Step 1: Temporary transform for stats ===\n",
    "    temp_transform = transforms.Compose([\n",
    "        transforms.Resize((180, 180)),  # Resize as required in coursework\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    dataset_temp = datasets.ImageFolder(root=data_dir, transform=temp_transform)\n",
    "    \n",
    "    # === Step 2: Split the dataset as required by coursework ===\n",
    "    total_length = len(dataset_temp)\n",
    "    train_len = int(0.7 * total_length)  # 70% training\n",
    "    val_len = int(0.2 * total_length)    # 20% validation\n",
    "    test_len = total_length - train_len - val_len  # 10% testing\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset_temp, [train_len, val_len, test_len],\n",
    "                                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    # === Step 3: Compute mean & std on training set ===\n",
    "    loader_for_stats = DataLoader(train_set, batch_size=batch_size)\n",
    "    mean, std, total_pixels = 0.0, 0.0, 0\n",
    "\n",
    "    for images, _ in loader_for_stats:\n",
    "        mean += images.sum().item()\n",
    "        std += (images ** 2).sum().item()\n",
    "        total_pixels += images.numel()\n",
    "\n",
    "    mean /= total_pixels\n",
    "    std = (std / total_pixels - mean ** 2) ** 0.5\n",
    "\n",
    "    print(f\"ðŸ“Š Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "\n",
    "    # === Step 4: Final transforms with data augmentation ===\n",
    "    transform_train = transforms.Compose([\n",
    "        # Random crop for data augmentation\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomCrop((80, 80))],\n",
    "            p=0.1\n",
    "        ),\n",
    "        # Resize to required dimensions\n",
    "        transforms.Resize((180, 180)),\n",
    "        # Convert to tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Random affine transform for data augmentation\n",
    "        transforms.RandomApply([transforms.RandomAffine(\n",
    "            degrees=0,\n",
    "            translate=(0.1, 0),\n",
    "            fill=0)],\n",
    "            p=0.1\n",
    "        ),\n",
    "        # Random erasing for data augmentation\n",
    "        transforms.RandomErasing(\n",
    "            p=0.5,\n",
    "            scale=(0.004, 0.02),\n",
    "            ratio=(20, 200)\n",
    "        ),\n",
    "        # Normalize with calculated mean and std\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "    # Simpler transform for validation and test sets\n",
    "    transform_val_test = transforms.Compose([\n",
    "        transforms.Resize((180, 180)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    # === Step 5: Reload dataset with new transforms ===\n",
    "    full_dataset = datasets.ImageFolder(root=data_dir, loader=custom_gray_loader)\n",
    "\n",
    "    # Apply transforms to splits\n",
    "    full_dataset.transform = transform_train\n",
    "    train_set.dataset = full_dataset\n",
    "\n",
    "    full_dataset.transform = transform_val_test\n",
    "    val_set.dataset = full_dataset\n",
    "    test_set.dataset = full_dataset\n",
    "\n",
    "    # === Step 6: Create dataloaders ===\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, mean, std\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        data_loader: DataLoader containing the dataset\n",
    "        device: Device to run evaluation on (CPU or GPU)\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy as a fraction [0, 1]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=50, early_stopping=False, patience=10):\n",
    "    \"\"\"\n",
    "    Train a model with optional early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer for training\n",
    "        loss_fn: Loss function\n",
    "        device: Device to train on (CPU or GPU)\n",
    "        epochs: Maximum number of epochs to train\n",
    "        early_stopping: Whether to use early stopping\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "        \n",
    "    Returns:\n",
    "        model: The trained model\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) if early_stopping else None\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "        print(f\"ðŸ“… Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Val Acc: {val_acc:.2%}\")\n",
    "\n",
    "        if early_stopping:\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"ðŸ›‘ Early stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "                break\n",
    "            \n",
    "    print(f\"Training complete. Best Val Acc: {best_val_acc:.2%}\")\n",
    "\n",
    "    if early_stopping and best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(f\"ðŸ” Highest Val Acc Achieved: {best_val_acc:.2%}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Prepare data for CNN models (1-4)\n",
    "data_path = \"./Data/greyscale_spectrograms\"\n",
    "train_loader, val_loader, test_loader, mean, std = get_dataloaders(data_path, batch_size=32)\n",
    "\n",
    "def show_images(loader, mean=0, std=1, num_images=12, figsize=(12, 9), denormalize=True):\n",
    "    \"\"\"\n",
    "    Display a grid of images from a dataloader.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader to get images from\n",
    "        mean: Mean used for normalization\n",
    "        std: Standard deviation used for normalization\n",
    "        num_images: Number of images to display\n",
    "        figsize: Figure size for the plot\n",
    "        denormalize: Whether to denormalize images before displaying\n",
    "    \"\"\"\n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(loader))\n",
    "    \n",
    "    # Create a grid of images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=figsize)\n",
    "    axes = axes.ravel()  # Flatten the grid to make indexing easier\n",
    "    \n",
    "    # Calculate how many images to actually display\n",
    "    num_to_display = min(num_images, len(images), len(axes))\n",
    "    \n",
    "    # Get class names from the dataset\n",
    "    class_names = loader.dataset.dataset.classes\n",
    "    \n",
    "    # Plot images\n",
    "    for idx in range(num_to_display):\n",
    "        # Get the image and label\n",
    "        img = images[idx]\n",
    "        label_idx = labels[idx].item()\n",
    "        class_name = class_names[label_idx]\n",
    "        \n",
    "        # Convert tensor to numpy for visualization\n",
    "        img_np = img.cpu().numpy()\n",
    "        \n",
    "        # Handle grayscale images\n",
    "        if img_np.shape[0] == 1:\n",
    "            img_np = img_np.squeeze(0)  # Remove the channel dimension\n",
    "            \n",
    "            # Denormalize if requested\n",
    "            if denormalize:\n",
    "                img_np = img_np * std + mean\n",
    "                \n",
    "            axes[idx].imshow(img_np, cmap='gray')\n",
    "        else:\n",
    "            # Handle RGB images\n",
    "            if denormalize:\n",
    "                img_np = img_np * std + mean\n",
    "                \n",
    "            img_np = np.transpose(img_np, (1, 2, 0))  # Change from [C,H,W] to [H,W,C]\n",
    "            axes[idx].imshow(img_np)\n",
    "        \n",
    "        # Add title and turn off axis\n",
    "        axes[idx].set_title(f'Genre: {class_name}', fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(num_to_display, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_names\n",
    "\n",
    "# Show sample images from the training set\n",
    "num_images = 10\n",
    "class_names = show_images(\n",
    "    train_loader, \n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    num_images=num_images, \n",
    "    figsize=(20, 4),\n",
    "    denormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0acb1",
   "metadata": {},
   "source": [
    "# Model 1 - Fully Connected Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1: Fully Connected Network with Two Hidden Layers\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected network with two hidden layers as required by coursework.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.fc1 = nn.Linear(180 * 180, 512)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(512, 128)        # Second hidden layer\n",
    "        self.out = nn.Linear(128, 10)         # Output layer (10 genres)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)     # Flatten input\n",
    "        x = self.relu(self.fc1(x))    # Hidden layer 1 + activation\n",
    "        x = self.relu(self.fc2(x))    # Hidden layer 2 + activation\n",
    "        return self.out(x)            # Output logits\n",
    "\n",
    "# Train Model 1 for 50 epochs as required\n",
    "print(\"\\n=== Training Model 1 (FCN) for 50 epochs ===\")\n",
    "model1_50 = Net1().to(device)\n",
    "optimizer1_50 = torch.optim.Adam(model1_50.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model1_50 = train_model(model1_50, train_loader, val_loader, optimizer1_50, loss_fn, device, epochs=50)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model1_50.state_dict(), 'Net1-50epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_1_50 = evaluate_accuracy(model1_50, train_loader, device)\n",
    "val_acc_1_50 = evaluate_accuracy(model1_50, val_loader, device)\n",
    "test_acc_1_50 = evaluate_accuracy(model1_50, test_loader, device)\n",
    "\n",
    "print(f\"Model 1 (50 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_1_50:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_1_50:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_1_50:.2%}\")\n",
    "\n",
    "# Train Model 1 for 100 epochs as required\n",
    "print(\"\\n=== Training Model 1 (FCN) for 100 epochs ===\")\n",
    "model1_100 = Net1().to(device)\n",
    "optimizer1_100 = torch.optim.Adam(model1_100.parameters(), lr=1e-3)\n",
    "\n",
    "model1_100 = train_model(model1_100, train_loader, val_loader, optimizer1_100, loss_fn, device, epochs=100)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model1_100.state_dict(), 'Net1-100epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_1_100 = evaluate_accuracy(model1_100, train_loader, device)\n",
    "val_acc_1_100 = evaluate_accuracy(model1_100, val_loader, device)\n",
    "test_acc_1_100 = evaluate_accuracy(model1_100, test_loader, device)\n",
    "\n",
    "print(f\"Model 1 (100 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_1_100:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_1_100:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_1_100:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37b072",
   "metadata": {},
   "source": [
    "# Model 2 - Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b927bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2: Convolutional Network with Custom Parameters\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional network as shown in Figure 1 with custom parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        # Conv block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  # [B,16,180,180]\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) # [B,32,180,180]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)                                # [B,32,90,90]\n",
    "        \n",
    "        # Conv block 2\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # [B,64,90,90]\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)# [B,128,90,90]\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)                                # [B,128,45,45]\n",
    "\n",
    "        # Fully connected\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 45 * 45, 256)  # Matches \"out_features=256\" in diagram\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train Model 2 for 50 epochs as required\n",
    "print(\"\\n=== Training Model 2 (CNN) for 50 epochs ===\")\n",
    "model2_50 = Net2().to(device)\n",
    "optimizer2_50 = torch.optim.Adam(model2_50.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model2_50 = train_model(model2_50, train_loader, val_loader, optimizer2_50, loss_fn, device, epochs=50)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model2_50.state_dict(), 'Net2-50epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_2_50 = evaluate_accuracy(model2_50, train_loader, device)\n",
    "val_acc_2_50 = evaluate_accuracy(model2_50, val_loader, device)\n",
    "test_acc_2_50 = evaluate_accuracy(model2_50, test_loader, device)\n",
    "\n",
    "print(f\"Model 2 (50 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_2_50:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_2_50:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_2_50:.2%}\")\n",
    "\n",
    "# Train Model 2 for 100 epochs as required\n",
    "print(\"\\n=== Training Model 2 (CNN) for 100 epochs ===\")\n",
    "model2_100 = Net2().to(device)\n",
    "optimizer2_100 = torch.optim.Adam(model2_100.parameters(), lr=1e-4)\n",
    "\n",
    "model2_100 = train_model(model2_100, train_loader, val_loader, optimizer2_100, loss_fn, device, epochs=100)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model2_100.state_dict(), 'Net2-100epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_2_100 = evaluate_accuracy(model2_100, train_loader, device)\n",
    "val_acc_2_100 = evaluate_accuracy(model2_100, val_loader, device)\n",
    "test_acc_2_100 = evaluate_accuracy(model2_100, test_loader, device)\n",
    "\n",
    "print(f\"Model 2 (100 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_2_100:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_2_100:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_2_100:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca995e22",
   "metadata": {},
   "source": [
    "# Model 3 - CNN with Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d732035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 3: CNN with Batch Normalization\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional network based on Model 2 but with batch normalization layers\n",
    "    as required by the coursework.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)  # Added batch normalization\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)  # Added batch normalization\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Block 2\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)  # Added batch normalization\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)  # Added batch normalization\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 45 * 45, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply batch normalization after each convolution\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train Model 3 for 50 epochs as required\n",
    "print(\"\\n=== Training Model 3 (CNN with BatchNorm) for 50 epochs ===\")\n",
    "model3_50 = Net3().to(device)\n",
    "optimizer3_50 = torch.optim.Adam(model3_50.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model3_50 = train_model(model3_50, train_loader, val_loader, optimizer3_50, loss_fn, device, epochs=50)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model3_50.state_dict(), 'Net3-50epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_3_50 = evaluate_accuracy(model3_50, train_loader, device)\n",
    "val_acc_3_50 = evaluate_accuracy(model3_50, val_loader, device)\n",
    "test_acc_3_50 = evaluate_accuracy(model3_50, test_loader, device)\n",
    "\n",
    "print(f\"Model 3 (50 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_3_50:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_3_50:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_3_50:.2%}\")\n",
    "\n",
    "# Train Model 3 for 100 epochs as required\n",
    "print(\"\\n=== Training Model 3 (CNN with BatchNorm) for 100 epochs ===\")\n",
    "model3_100 = Net3().to(device)\n",
    "optimizer3_100 = torch.optim.Adam(model3_100.parameters(), lr=1e-4)\n",
    "\n",
    "model3_100 = train_model(model3_100, train_loader, val_loader, optimizer3_100, loss_fn, device, epochs=100)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model3_100.state_dict(), 'Net3-100epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_3_100 = evaluate_accuracy(model3_100, train_loader, device)\n",
    "val_acc_3_100 = evaluate_accuracy(model3_100, val_loader, device)\n",
    "test_acc_3_100 = evaluate_accuracy(model3_100, test_loader, device)\n",
    "\n",
    "print(f\"Model 3 (100 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_3_100:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_3_100:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_3_100:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb499ef",
   "metadata": {},
   "source": [
    "# Model 4 - CNN with Batch Normalization and RMSprop Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9acc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 4: CNN with Batch Normalization and RMSprop Optimizer\n",
    "\n",
    "class Net4(nn.Module):\n",
    "    \"\"\"\n",
    "    Same architecture as Model 3 but using RMSprop optimizer as required by coursework.\n",
    "    Model is defined again for clarity, although it's identical to Net3.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Block 2\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 45 * 45, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Train Model 4 for 50 epochs as required - using RMSprop optimizer\n",
    "print(\"\\n=== Training Model 4 (CNN with BatchNorm and RMSprop) for 50 epochs ===\")\n",
    "model4_50 = Net4().to(device)\n",
    "# The key difference: using RMSprop instead of Adam\n",
    "optimizer4_50 = torch.optim.RMSprop(model4_50.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model4_50 = train_model(model4_50, train_loader, val_loader, optimizer4_50, loss_fn, device, epochs=50)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model4_50.state_dict(), 'Net4-50epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_4_50 = evaluate_accuracy(model4_50, train_loader, device)\n",
    "val_acc_4_50 = evaluate_accuracy(model4_50, val_loader, device)\n",
    "test_acc_4_50 = evaluate_accuracy(model4_50, test_loader, device)\n",
    "\n",
    "print(f\"Model 4 (50 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_4_50:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_4_50:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_4_50:.2%}\")\n",
    "\n",
    "# Train Model 4 for 100 epochs as required\n",
    "print(\"\\n=== Training Model 4 (CNN with BatchNorm and RMSprop) for 100 epochs ===\")\n",
    "model4_100 = Net4().to(device)\n",
    "optimizer4_100 = torch.optim.RMSprop(model4_100.parameters(), lr=1e-4)\n",
    "\n",
    "model4_100 = train_model(model4_100, train_loader, val_loader, optimizer4_100, loss_fn, device, epochs=100)\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model4_100.state_dict(), 'Net4-100epoch.pth')\n",
    "\n",
    "# Evaluate performance\n",
    "train_acc_4_100 = evaluate_accuracy(model4_100, train_loader, device)\n",
    "val_acc_4_100 = evaluate_accuracy(model4_100, val_loader, device)\n",
    "test_acc_4_100 = evaluate_accuracy(model4_100, test_loader, device)\n",
    "\n",
    "print(f\"Model 4 (100 epochs) Results:\")\n",
    "print(f\"Train Accuracy: {train_acc_4_100:.2%}\")\n",
    "print(f\"Validation Accuracy: {val_acc_4_100:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc_4_100:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf26b8",
   "metadata": {},
   "source": [
    "# Audio Dataset for Models 5-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ca624",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio Dataset for LSTM Models (5-6)\n",
    "\n",
    "# MFCC Dataset for audio features used in RNN models\n",
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, root_dir, n_mfcc=40, max_len=400, include_delta=True, \n",
    "                 include_energy=True, n_fft=2048, hop_length=512, sample_rate=None, \n",
    "                 transform=None, scaler=None, train_mode=True):\n",
    "        \"\"\"\n",
    "        Enhanced MFCC Dataset with additional features\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Directory with audio files organized by genre\n",
    "            n_mfcc (int): Number of MFCC coefficients\n",
    "            max_len (int): Maximum sequence length (samples shorter will be padded)\n",
    "            include_delta (bool): Whether to include delta and delta-delta features\n",
    "            include_energy (bool): Whether to include energy as first MFCC\n",
    "            n_fft (int): FFT window size\n",
    "            hop_length (int): Hop length for feature extraction\n",
    "            sample_rate (int, optional): Target sample rate, None keeps original\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            scaler (object, optional): Pre-fit scaler to apply normalization\n",
    "            train_mode (bool): Whether to fit a new scaler on this dataset\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.max_len = max_len\n",
    "        self.include_delta = include_delta\n",
    "        self.include_energy = include_energy\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        self.scaler = scaler\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        # Stats for tracking\n",
    "        self.total_length = 0\n",
    "        self.min_length = float('inf')\n",
    "        self.max_length = 0\n",
    "        \n",
    "        # Load data with progress bar\n",
    "        self._load_data()\n",
    "    \n",
    "    def _extract_features(self, y, sr):\n",
    "        \"\"\"Extract MFCCs with additional delta features if requested\"\"\"\n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=y, sr=sr, \n",
    "            n_mfcc=self.n_mfcc, \n",
    "            n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length, \n",
    "            htk=True,  # Use HTK formula for better compatibility with standards\n",
    "        )\n",
    "        \n",
    "        # Add delta and delta-delta features if requested\n",
    "        if self.include_delta:\n",
    "            delta_mfccs = librosa.feature.delta(mfccs)\n",
    "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "            features = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
    "        else:\n",
    "            features = mfccs\n",
    "        \n",
    "        # Add energy feature if requested\n",
    "        if self.include_energy:\n",
    "            # Calculate energy\n",
    "            energy = librosa.feature.rms(y=y, frame_length=self.n_fft, hop_length=self.hop_length)\n",
    "            features = np.concatenate([energy, features], axis=0)\n",
    "        \n",
    "        features = features.T  # Convert to time-major format [time, features]\n",
    "        \n",
    "        # Update stats\n",
    "        length = features.shape[0]\n",
    "        self.total_length += length\n",
    "        self.min_length = min(self.min_length, length)\n",
    "        self.max_length = max(self.max_length, length)\n",
    "        \n",
    "        # Handle sequence length: pad or truncate\n",
    "        if length > self.max_len:\n",
    "            features = features[:self.max_len, :]\n",
    "        elif length < self.max_len:\n",
    "            # Pad with zeros\n",
    "            padding = np.zeros((self.max_len - length, features.shape[1]))\n",
    "            features = np.concatenate([features, padding], axis=0)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load audio files and extract features\"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.filenames = []  # Keep track of filenames for debugging\n",
    "        \n",
    "        # Collect genre names based on folders\n",
    "        self.genres = []\n",
    "        for item in os.listdir(self.root_dir):\n",
    "            if os.path.isdir(os.path.join(self.root_dir, item)):\n",
    "                self.genres.append(item)\n",
    "        self.genres.sort()  # Ensure consistent genre ordering\n",
    "        \n",
    "        genre_to_idx = {genre: idx for idx, genre in enumerate(self.genres)}\n",
    "        all_features = []\n",
    "        \n",
    "        print(f\"Loading audio from {self.root_dir}\")\n",
    "        print(f\"Found {len(self.genres)} genres: {', '.join(self.genres)}\")\n",
    "        \n",
    "        for genre_name in self.genres:\n",
    "            genre_dir = os.path.join(self.root_dir, genre_name)\n",
    "            label_idx = genre_to_idx[genre_name]\n",
    "            \n",
    "            file_list = [f for f in os.listdir(genre_dir) \n",
    "                        if f.endswith('.wav') or f.endswith('.mp3')]\n",
    "            \n",
    "            print(f\"Processing {len(file_list)} files for genre '{genre_name}'\")\n",
    "            \n",
    "            for filename in tqdm(file_list, desc=f\"Genre: {genre_name}\"):\n",
    "                try:\n",
    "                    file_path = os.path.join(genre_dir, filename)\n",
    "                    \n",
    "                    # Load and extract features\n",
    "                    y, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "                    \n",
    "                    # Apply a simple audio quality check\n",
    "                    if np.isnan(y).any() or np.max(np.abs(y)) < 1e-6:\n",
    "                        print(f\"Warning: {file_path} has audio quality issues. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract MFCCs and additional features\n",
    "                    features = self._extract_features(y, sr)\n",
    "                    \n",
    "                    all_features.append(features)\n",
    "                    self.filenames.append(filename)\n",
    "                    self.labels.append(label_idx)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "        \n",
    "        # Convert to numpy arrays for processing\n",
    "        all_features = np.array(all_features)\n",
    "        \n",
    "        # Fit scaler if in training mode and no scaler provided\n",
    "        feature_dim = all_features.shape[2]\n",
    "        if self.train_mode and self.scaler is None:\n",
    "            # Reshape for fitting the scaler\n",
    "            flat_features = all_features.reshape(-1, feature_dim)\n",
    "            self.scaler = StandardScaler().fit(flat_features)\n",
    "            print(f\"Fitted scaler with mean: {self.scaler.mean_.mean():.4f}, std: {self.scaler.scale_.mean():.4f}\")\n",
    "        \n",
    "        # Apply scaling if scaler is available\n",
    "        if self.scaler is not None:\n",
    "            # Reshape, transform, and reshape back\n",
    "            orig_shape = all_features.shape\n",
    "            flat_features = all_features.reshape(-1, feature_dim)\n",
    "            scaled_features = self.scaler.transform(flat_features)\n",
    "            all_features = scaled_features.reshape(orig_shape)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        self.data = [torch.tensor(features, dtype=torch.float32) for features in all_features]\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Dataset loading complete. {len(self.data)} samples, {feature_dim} features.\")\n",
    "        print(f\"Sequence length stats - Min: {self.min_length}, Max: {self.max_length}, Used: {self.max_len}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "            \n",
    "        return features, label\n",
    "\n",
    "def get_mfcc_dataloaders(data_path, batch_size=32, n_mfcc=40, max_len=400, \n",
    "                         include_delta=True, include_energy=True, \n",
    "                         n_fft=2048, hop_length=512):\n",
    "    \"\"\"Create train, validation, and test data loaders with consistent preprocessing\"\"\"\n",
    "    \n",
    "    # Create the training dataset\n",
    "    train_dataset = MFCCDataset(\n",
    "        root_dir=data_path,\n",
    "        n_mfcc=n_mfcc,\n",
    "        max_len=max_len,\n",
    "        include_delta=include_delta,\n",
    "        include_energy=include_energy,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        train_mode=True  # This will fit a scaler\n",
    "    )\n",
    "    \n",
    "    # Get the genres\n",
    "    genres = train_dataset.genres\n",
    "    scaler = train_dataset.scaler\n",
    "    \n",
    "    # Calculate total length for dataset splits\n",
    "    total_len = len(train_dataset)\n",
    "    train_len = int(0.7 * total_len)  # 70% training\n",
    "    val_len = int(0.2 * total_len)    # 20% validation\n",
    "    test_len = total_len - train_len - val_len  # 10% testing\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_set, val_set, test_set = random_split(\n",
    "        train_dataset, \n",
    "        [train_len, val_len, test_len],\n",
    "        generator=torch.Generator().manual_seed(42)  # Fixed seed for reproducibility\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, genres, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164becf1",
   "metadata": {},
   "source": [
    "# Model 5 - LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 5: LSTM with Attention\n",
    "\n",
    "# Attention mechanism for LSTM\n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism to focus on important timesteps in sequence\"\"\"\n",
    "    def __init__(self, hidden_size, attention_size=128):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, attention_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [batch, seq_len, hidden_size]\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(hidden_states)  # [batch, seq_len, 1]\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # [batch, seq_len, 1]\n",
    "        \n",
    "        # Apply attention weights to get context vector\n",
    "        context = torch.sum(hidden_states * attention_weights, dim=1)  # [batch, hidden_size]\n",
    "        return context, attention_weights\n",
    "\n",
    "class Net5(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model with attention mechanism for audio classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=40, hidden_size=256, num_layers=2, num_classes=10, \n",
    "                 dropout=0.3, bidirectional=True, use_attention=True):\n",
    "        super(Net5, self).__init__()\n",
    "        \n",
    "        # Whether to use bidirectional LSTM\n",
    "        self.bidirectional = bidirectional\n",
    "        # Whether to use attention mechanism\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate the output size from LSTM\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention layer if requested\n",
    "        if use_attention:\n",
    "            self.attention = AttentionLayer(lstm_output_size)\n",
    "        \n",
    "        # Fully connected layers with batch normalization\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: [batch, seq_len, hidden_size(*2 if bidirectional)]\n",
    "        \n",
    "        # Apply attention if requested, otherwise use the last hidden state\n",
    "        if self.use_attention:\n",
    "            context, attention_weights = self.attention(lstm_out)\n",
    "        else:\n",
    "            if self.bidirectional:\n",
    "                # When bidirectional, concatenate the last hidden state from both directions\n",
    "                context = torch.cat([lstm_out[:, -1, :lstm_out.size(2)//2], \n",
    "                                    lstm_out[:, 0, lstm_out.size(2)//2:]], dim=1)\n",
    "            else:\n",
    "                # Use the last hidden state when not bidirectional\n",
    "                context = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.bn1(self.fc1(context)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "# Helper function to count parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def train_lstm_model(model, train_loader, val_loader, optimizer, scheduler=None, \n",
    "                     loss_fn=nn.CrossEntropyLoss(), device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), \n",
    "                     epochs=50, patience=10, checkpoint_path='best_model.pt'):\n",
    "    \"\"\"\n",
    "    Training function with early stopping and learning rate scheduling\n",
    "    \"\"\"\n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=checkpoint_path)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate training statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                # Calculate validation statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'Epoch: {epoch+1}/{epochs} | '\n",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)  # For ReduceLROnPlateau\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test set\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Classification accuracy\n",
    "        confusion (numpy.ndarray): Confusion matrix\n",
    "        class_accuracies (list): Per-class accuracies\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = []\n",
    "    for i in range(len(conf_mat)):\n",
    "        if sum(conf_mat[i, :]) > 0:  # Avoid division by zero\n",
    "            per_class_accuracy.append(conf_mat[i, i] / sum(conf_mat[i, :]) * 100)\n",
    "        else:\n",
    "            per_class_accuracy.append(0)\n",
    "    \n",
    "    return accuracy, conf_mat, per_class_accuracy\n",
    "\n",
    "# Train Model 5 (LSTM)\n",
    "print(\"\\n=== Training Model 5 (RNN with LSTMs) ===\")\n",
    "\n",
    "# Set hyperparameters\n",
    "n_mfcc = 20             # Number of MFCC coefficients\n",
    "max_len = 300           # Max sequence length\n",
    "include_delta = True    # Include delta and delta-delta features\n",
    "include_energy = True   # Include energy as the first MFCC\n",
    "n_fft = 2048            # FFT window size\n",
    "hop_length = 512        # Hop length\n",
    "batch_size = 32         # Batch size\n",
    "hidden_size = 128       # LSTM hidden size\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "dropout = 0.4           # Dropout rate\n",
    "learning_rate = 0.001   # Learning rate\n",
    "epochs = 80             # Max epochs\n",
    "patience = 15           # Early stopping patience\n",
    "bidirectional = True    # Use bidirectional LSTM\n",
    "use_attention = True    # Use attention mechanism\n",
    "\n",
    "# Load audio dataset\n",
    "data_path = \"./Data/genres_original\"  # Path to your audio files\n",
    "\n",
    "print(\"Loading and preprocessing audio data...\")\n",
    "train_loader, val_loader, test_loader, genres, scaler = get_mfcc_dataloaders(\n",
    "    data_path=data_path,\n",
    "    batch_size=batch_size,\n",
    "    n_mfcc=n_mfcc,\n",
    "    max_len=max_len,\n",
    "    include_delta=include_delta,\n",
    "    include_energy=include_energy,\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length\n",
    ")\n",
    "\n",
    "# Calculate input size based on features\n",
    "sample_batch, _ = next(iter(train_loader))\n",
    "input_size = sample_batch.size(-1)\n",
    "print(f\"Input feature size: {input_size}\")\n",
    "\n",
    "# Initialize model\n",
    "model5 = Net5(\n",
    "    input_size=input_size, \n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=len(genres),\n",
    "    dropout=dropout,\n",
    "    bidirectional=bidirectional,\n",
    "    use_attention=use_attention\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model5):,}\")\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model5.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model with early stopping until convergence\n",
    "print(\"Training LSTM model...\")\n",
    "model5, history = train_lstm_model(\n",
    "    model=model5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    checkpoint_path=\"best_net5_model.pt\"\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy, confusion_mat, per_class_acc = evaluate_model(model5, test_loader, device)\n",
    "print(f\"Highest val_acc: {max(history['val_acc']):.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(\"Per-class accuracy:\")\n",
    "for i, acc in enumerate(per_class_acc):\n",
    "    print(f\"  {genres[i]}: {acc:.2f}%\")\n",
    "\n",
    "# Create confusion matrix visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "conf_df = pd.DataFrame(confusion_mat, index=genres, columns=genres)\n",
    "sns.heatmap(conf_df, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "plt.title(\"Confusion Matrix - Model 5 (LSTM)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net5_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train')\n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train')\n",
    "plt.plot(history['val_acc'], label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net5_training_history.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model5.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'genres': genres,\n",
    "    'input_size': input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers': num_layers,\n",
    "    'bidirectional': bidirectional,\n",
    "    'use_attention': use_attention,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'confusion_matrix': confusion_mat.tolist(),\n",
    "    'per_class_accuracy': per_class_acc,\n",
    "}, 'net5_full_model.pth')\n",
    "\n",
    "print(\"Model 5 and training results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4501aa6",
   "metadata": {},
   "source": [
    "# Model 6 - LSTM with GAN Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 6: RNN with LSTMs and GANs for Data Augmentation\n",
    "\n",
    "# GAN Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, label_dim=10, output_dim=60, seq_len=300):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.label_dim = label_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embedding = nn.Embedding(label_dim, 50)\n",
    "        \n",
    "        # Main network\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(noise_dim + 50, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Linear(1024, seq_len * output_dim),\n",
    "            nn.Tanh()  # Output range: [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Embed labels\n",
    "        label_embedding = self.label_embedding(labels)\n",
    "        \n",
    "        # Concatenate noise and label embedding\n",
    "        x = torch.cat([z, label_embedding], dim=1)\n",
    "        \n",
    "        # Generate data\n",
    "        x = self.main(x)\n",
    "        \n",
    "        # Reshape to sequence format [batch, seq_len, features]\n",
    "        x = x.view(x.size(0), self.seq_len, self.output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# GAN Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=60, seq_len=300, label_dim=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embedding = nn.Embedding(label_dim, 50)\n",
    "        \n",
    "        # Main network\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten the sequence\n",
    "            nn.Linear(seq_len * input_dim + 50, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        # x input shape: [batch, seq_len, input_dim]\n",
    "        \n",
    "        # Embed labels\n",
    "        label_embedding = self.label_embedding(labels)\n",
    "        \n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Concatenate with label embedding\n",
    "        x = torch.cat([x, label_embedding], dim=1)\n",
    "        \n",
    "        # Discriminator output\n",
    "        return self.main(x)\n",
    "\n",
    "# Weight initialization for GAN models\n",
    "def weights_init(m):\n",
    "    \"\"\"Initialize network weights for better GAN training\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 or classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# GAN Training function\n",
    "def train_gan(generator, discriminator, dataloader, epochs=60, noise_dim=100,\n",
    "             batch_size=32, device=device, save_dir=\"gan_checkpoints\"):\n",
    "    \"\"\"Train GAN for data augmentation\"\"\"\n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Set up optimizers\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Fixed noise for visualization\n",
    "    fixed_noise = torch.randn(10, noise_dim).to(device)\n",
    "    fixed_labels = torch.arange(0, 10).to(device)\n",
    "    \n",
    "    # Training metrics\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define real and fake labels for loss calculation\n",
    "    real_label = 1.0\n",
    "    fake_label = 0.0\n",
    "    \n",
    "    print(\"Starting GAN training...\")\n",
    "    for epoch in range(epochs):\n",
    "        g_loss_epoch = 0.0\n",
    "        d_loss_epoch = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for real_data, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_size = real_data.size(0)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Move data to device\n",
    "            real_data = real_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_d.zero_grad()\n",
    "            \n",
    "            # Train with real data\n",
    "            real_target = torch.full((batch_size, 1), real_label, dtype=torch.float, device=device)\n",
    "            real_output = discriminator(real_data, labels)\n",
    "            d_loss_real = criterion(real_output, real_target)\n",
    "            d_loss_real.backward()\n",
    "            \n",
    "            # Train with fake data\n",
    "            noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "            fake_data = generator(noise, labels)\n",
    "            fake_target = torch.full((batch_size, 1), fake_label, dtype=torch.float, device=device)\n",
    "            fake_output = discriminator(fake_data.detach(), labels)\n",
    "            d_loss_fake = criterion(fake_output, fake_target)\n",
    "            d_loss_fake.backward()\n",
    "            \n",
    "            # Combined discriminator loss\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            # Generator wants discriminator to think fake data is real\n",
    "            output = discriminator(fake_data, labels)\n",
    "            g_loss = criterion(output, real_target)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            # Track losses\n",
    "            g_loss_epoch += g_loss.item()\n",
    "            d_loss_epoch += d_loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        g_loss_epoch /= batch_count\n",
    "        d_loss_epoch /= batch_count\n",
    "        g_losses.append(g_loss_epoch)\n",
    "        d_losses.append(d_loss_epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "              f\"G Loss: {g_loss_epoch:.4f} | D Loss: {d_loss_epoch:.4f} | \"\n",
    "              f\"Time: {elapsed/60:.1f}m\")\n",
    "        \n",
    "        # Save samples and checkpoints\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            # Generate samples\n",
    "            with torch.no_grad():\n",
    "                generator.eval()\n",
    "                samples = generator(fixed_noise, fixed_labels)\n",
    "                generator.train()\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'g_loss': g_loss_epoch,\n",
    "                'd_loss': d_loss_epoch\n",
    "            }, os.path.join(save_dir, f\"gan_checkpoint_epoch_{epoch+1}.pt\"))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('GAN Training Losses')\n",
    "    plt.savefig(os.path.join(save_dir, \"gan_training_loss.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict()\n",
    "    }, os.path.join(save_dir, \"gan_final.pt\"))\n",
    "    \n",
    "    print(f\"GAN training completed in {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    \n",
    "    return generator, discriminator, g_losses, d_losses\n",
    "\n",
    "# Generate synthetic data using trained GAN\n",
    "def generate_synthetic_data(generator, num_samples_per_class=100, num_classes=10, \n",
    "                           noise_dim=100, device=device):\n",
    "    \"\"\"Generate synthetic data using the trained generator\"\"\"\n",
    "    generator.eval()\n",
    "    generator.to(device)\n",
    "    \n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Generate samples for each class\n",
    "    for class_idx in range(num_classes):\n",
    "        # Create labels tensor\n",
    "        labels = torch.full((num_samples_per_class,), class_idx, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Create noise vector\n",
    "        z = torch.randn(num_samples_per_class, noise_dim).to(device)\n",
    "        \n",
    "        # Generate samples\n",
    "        with torch.no_grad():\n",
    "            fake_samples = generator(z, labels)\n",
    "        \n",
    "        # Store samples and labels\n",
    "        all_samples.append(fake_samples.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all samples and labels\n",
    "    all_samples = torch.cat(all_samples, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    return all_samples, all_labels\n",
    "\n",
    "# LSTM Model for Net6 (same architecture as Net5)\n",
    "class Net6(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN with LSTM layers for audio classification (architecture is identical to Net5)\n",
    "    This model will be trained with GAN-augmented data\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=60, hidden_size=256, num_layers=2, num_classes=10, \n",
    "                 dropout=0.4, bidirectional=True):\n",
    "        super(Net6, self).__init__()\n",
    "        \n",
    "        # LSTM parameters\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate LSTM output size\n",
    "        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(lstm_output_size, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(lstm_output_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def apply_attention(self, lstm_output):\n",
    "        \"\"\"Apply attention mechanism to focus on important time steps\"\"\"\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(lstm_output)  # (batch_size, seq_len, 1)\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Apply softmax over sequence length\n",
    "        \n",
    "        # Apply attention weights to get context vector\n",
    "        context = torch.sum(lstm_output * attn_weights, dim=1)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        context = self.apply_attention(lstm_out)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn1(self.fc1(context)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create augmented dataset using GAN\n",
    "def create_augmented_dataset(real_dataset, gan_generator, num_samples_per_class=100,\n",
    "                            noise_dim=100, device=device):\n",
    "    \"\"\"Create a dataset augmented with GAN-generated samples\"\"\"\n",
    "    # Get real data\n",
    "    real_data = []\n",
    "    real_labels = []\n",
    "    \n",
    "    for i in range(len(real_dataset)):\n",
    "        features, label = real_dataset[i]\n",
    "        real_data.append(features)\n",
    "        real_labels.append(label)\n",
    "    \n",
    "    real_data = torch.stack(real_data)\n",
    "    real_labels = torch.tensor(real_labels)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    print(f\"Generating {num_samples_per_class} synthetic samples per class...\")\n",
    "    synthetic_data, synthetic_labels = generate_synthetic_data(\n",
    "        generator=gan_generator,\n",
    "        num_samples_per_class=num_samples_per_class,\n",
    "        num_classes=len(real_dataset.genres),\n",
    "        noise_dim=noise_dim,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    real_tensor_dataset = TensorDataset(real_data, real_labels)\n",
    "    synthetic_dataset = TensorDataset(synthetic_data, synthetic_labels)\n",
    "    \n",
    "    # Combine datasets\n",
    "    augmented_dataset = ConcatDataset([real_tensor_dataset, synthetic_dataset])\n",
    "    \n",
    "    print(f\"Augmented dataset created with {len(real_tensor_dataset)} real samples and \"\n",
    "          f\"{len(synthetic_dataset)} synthetic samples\")\n",
    "    \n",
    "    return augmented_dataset, real_data, real_labels, synthetic_data, synthetic_labels\n",
    "\n",
    "# Complete training pipeline for Net6 with GAN augmentation\n",
    "def train_net6_with_gan(data_path=\"./Data/genres_original\", num_samples_per_class=100):\n",
    "    \"\"\"Train Net6 with GAN data augmentation\"\"\"\n",
    "    # Create output directories\n",
    "    os.makedirs(\"gan_checkpoints\", exist_ok=True)\n",
    "    \n",
    "    # Step 1: Load dataset\n",
    "    print(\"Loading MFCC dataset...\")\n",
    "    dataset = MFCCDataset(\n",
    "        root_dir=data_path,\n",
    "        n_mfcc=n_mfcc,\n",
    "        max_len=max_len,\n",
    "        include_delta=True,\n",
    "        include_energy=True,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        sample_rate=None,\n",
    "        train_mode=True\n",
    "    )\n",
    "    \n",
    "    genres = dataset.genres\n",
    "    print(f\"Genres: {', '.join(genres)}\")\n",
    "    \n",
    "    # Determine input dimensions\n",
    "    sample_data, _ = dataset[0]\n",
    "    input_dim = sample_data.shape[1]  # Feature dimension\n",
    "    \n",
    "    print(f\"Input dimensions: sequence length = {max_len}, features = {input_dim}\")\n",
    "    \n",
    "    # Create data loader for GAN training\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Step 2: Train GAN\n",
    "    print(\"\\nTraining GAN for data augmentation...\")\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    NOISE_DIM = 100\n",
    "    GAN_EPOCHS = 60\n",
    "    \n",
    "    # Create GAN models\n",
    "    generator = Generator(\n",
    "        noise_dim=NOISE_DIM,\n",
    "        label_dim=len(genres),\n",
    "        output_dim=input_dim,\n",
    "        seq_len=max_len\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = Discriminator(\n",
    "        input_dim=input_dim,\n",
    "        seq_len=max_len,\n",
    "        label_dim=len(genres)\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init)\n",
    "    discriminator.apply(weights_init)\n",
    "    \n",
    "    # Train GAN\n",
    "    generator, discriminator, _, _ = train_gan(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        dataloader=dataloader,\n",
    "        epochs=GAN_EPOCHS,\n",
    "        noise_dim=NOISE_DIM,\n",
    "        batch_size=batch_size,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create augmented dataset\n",
    "    print(\"\\nCreating augmented dataset...\")\n",
    "    \n",
    "    augmented_dataset, _, _, _, _ = create_augmented_dataset(\n",
    "        real_dataset=dataset,\n",
    "        gan_generator=generator,\n",
    "        num_samples_per_class=num_samples_per_class,\n",
    "        noise_dim=NOISE_DIM,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Step 4: Train Net6\n",
    "    print(\"\\nTraining Net6 with augmented data...\")\n",
    "    \n",
    "    # Split dataset\n",
    "    total_size = len(augmented_dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.2 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        augmented_dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create Net6 model\n",
    "    net6 = Net6(\n",
    "        input_size=input_dim,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=len(genres),\n",
    "        dropout=dropout,\n",
    "        bidirectional=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(net6.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train model\n",
    "    net6, history = train_lstm_model(\n",
    "        model=net6,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        checkpoint_path=\"net6_best.pt\"\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train')\n",
    "    plt.plot(history['val_acc'], label='Validation')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"net6_training_history.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating Net6 on test set:\")\n",
    "    test_acc, cm, per_class_acc = evaluate_model(net6, test_loader, device)\n",
    "    \n",
    "    # Create confusion matrix visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    conf_df = pd.DataFrame(cm, index=genres, columns=genres)\n",
    "    sns.heatmap(conf_df, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix - Model 6 (LSTM + GAN)\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('net6_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print per-class accuracy\n",
    "    print(\"Per-class accuracy:\")\n",
    "    for i, genre in enumerate(genres):\n",
    "        print(f\"{genre}: {per_class_acc[i]:.2f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'model_state_dict': net6.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'test_accuracy': test_acc,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'genres': genres,\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    torch.save(results, \"net6_results.pt\")\n",
    "    \n",
    "    print(f\"Net6 training complete. Final test accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'model': net6,\n",
    "        'test_accuracy': test_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'per_class_accuracy': per_class_acc,\n",
    "        'history': history,\n",
    "        'genres': genres\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb7e04",
   "metadata": {},
   "source": [
    "# Model Comparison and Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0860f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Net5 and Net6 performance\n",
    "def compare_net5_net6(net5_path=\"net5_full_model.pth\", net6_path=\"net6_results.pt\"):\n",
    "    \"\"\"Compare the performance of Net5 and Net6\"\"\"\n",
    "    try:\n",
    "        # Load results\n",
    "        net5_results = torch.load(net5_path)\n",
    "        net6_results = torch.load(net6_path)\n",
    "        \n",
    "        # Extract accuracies\n",
    "        net5_acc = net5_results['test_accuracy']\n",
    "        net6_acc = net6_results['test_accuracy']\n",
    "        \n",
    "        # Extract per-class accuracies\n",
    "        if isinstance(net5_results['per_class_accuracy'], list):\n",
    "            net5_per_class = np.array(net5_results['per_class_accuracy'])\n",
    "        else:\n",
    "            net5_per_class = net5_results['per_class_accuracy']\n",
    "            \n",
    "        if isinstance(net6_results['per_class_accuracy'], list):\n",
    "            net6_per_class = np.array(net6_results['per_class_accuracy'])\n",
    "        else:\n",
    "            net6_per_class = net6_results['per_class_accuracy']\n",
    "        \n",
    "        # Get genres\n",
    "        genres = net5_results['genres']\n",
    "        \n",
    "        # Plot overall accuracy comparison\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(['Net5 (LSTM)', 'Net6 (LSTM+GAN)'], [net5_acc, net6_acc])\n",
    "        plt.title('Overall Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.ylim(0, 100)\n",
    "        \n",
    "        # Plot per-class accuracy comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        x = np.arange(len(genres))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, net5_per_class, width, label='Net5 (LSTM)')\n",
    "        plt.bar(x + width/2, net6_per_class, width, label='Net6 (LSTM+GAN)')\n",
    "        \n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Per-class Accuracy Comparison')\n",
    "        plt.xticks(x, genres, rotation=45)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('net5_vs_net6_comparison.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print improvement statistics\n",
    "        print(\"\\nPerformance Comparison:\")\n",
    "        print(f\"Overall: Net5 = {net5_acc:.2f}%, Net6 = {net6_acc:.2f}%\")\n",
    "        print(f\"Difference: {net6_acc - net5_acc:.2f}%\")\n",
    "        \n",
    "        print(\"\\nPer-class accuracy differences (Net6 - Net5):\")\n",
    "        for i, genre in enumerate(genres):\n",
    "            diff = net6_per_class[i] - net5_per_class[i]\n",
    "            print(f\"{genre}: {diff:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'net5_acc': net5_acc,\n",
    "            'net6_acc': net6_acc,\n",
    "            'net5_per_class': net5_per_class,\n",
    "            'net6_per_class': net6_per_class,\n",
    "            'genres': genres\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Make sure both Net5 and Net6 result files exist.\")\n",
    "        return None\n",
    "\n",
    "# Train Model 6\n",
    "print(\"\\n=== Training Model 6 (LSTM with GAN Augmentation) ===\")\n",
    "net6_results = train_net6_with_gan(\n",
    "    data_path=\"./Data/genres_original\",\n",
    "    num_samples_per_class=100\n",
    ")\n",
    "\n",
    "# Compare Net5 and Net6 performance\n",
    "print(\"\\n=== Comparing Model 5 (LSTM) and Model 6 (LSTM with GAN) ===\")\n",
    "comparison_results = compare_net5_net6()\n",
    "\n",
    "## Final Model Performance Comparison\n",
    "\n",
    "# Create a table to compare all models\n",
    "print(\"\\n=== Final Comparison of All Models ===\")\n",
    "\n",
    "# Collect all model results\n",
    "models_50_epochs = {\n",
    "    \"Model 1 (FCN)\": test_acc_1_50,\n",
    "    \"Model 2 (CNN)\": test_acc_2_50,\n",
    "    \"Model 3 (CNN+BN)\": test_acc_3_50,\n",
    "    \"Model 4 (CNN+BN+RMSprop)\": test_acc_4_50,\n",
    "}\n",
    "\n",
    "models_100_epochs = {\n",
    "    \"Model 1 (FCN)\": test_acc_1_100,\n",
    "    \"Model 2 (CNN)\": test_acc_2_100,\n",
    "    \"Model 3 (CNN+BN)\": test_acc_3_100,\n",
    "    \"Model 4 (CNN+BN+RMSprop)\": test_acc_4_100,\n",
    "}\n",
    "\n",
    "# Include LSTM models\n",
    "try:\n",
    "    lstm_models = {\n",
    "        \"Model 5 (LSTM)\": net5_results['test_accuracy'], \n",
    "        \"Model 6 (LSTM+GAN)\": net6_results['test_accuracy']\n",
    "    }\n",
    "except:\n",
    "    # If Model 5 or 6 results aren't available\n",
    "    lstm_models = {}\n",
    "\n",
    "# Create pandas dataframe for visualization\n",
    "models_data = []\n",
    "\n",
    "# Add CNN models with 50 epochs\n",
    "for model_name, acc in models_50_epochs.items():\n",
    "    models_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Epochs\": 50,\n",
    "        \"Test Accuracy (%)\": acc * 100\n",
    "    })\n",
    "\n",
    "# Add CNN models with 100 epochs\n",
    "for model_name, acc in models_100_epochs.items():\n",
    "    models_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Epochs\": 100, \n",
    "        \"Test Accuracy (%)\": acc * 100\n",
    "    })\n",
    "\n",
    "# Add LSTM models\n",
    "for model_name, acc in lstm_models.items():\n",
    "    models_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Epochs\": \"Until convergence\",\n",
    "        \"Test Accuracy (%)\": acc\n",
    "    })\n",
    "\n",
    "# Create dataframe and display\n",
    "results_df = pd.DataFrame(models_data)\n",
    "print(results_df)\n",
    "\n",
    "# Visualization of all model performances\n",
    "plt.figure(figsize=(14, 8))\n",
    "barplot_data = results_df.copy()\n",
    "\n",
    "# For LSTM models, create separate category\n",
    "barplot_data['Type'] = barplot_data['Model'].apply(\n",
    "    lambda x: 'LSTM Models' if 'LSTM' in x else f\"CNN Models ({barplot_data.loc[barplot_data['Model']==x, 'Epochs'].values[0]} epochs)\"\n",
    ")\n",
    "\n",
    "# Create bar plot\n",
    "sns.barplot(data=barplot_data, x='Model', y='Test Accuracy (%)', hue='Type')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Music Genre Classification: Model Performance Comparison')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae013403",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\"\"\"\n",
    "This notebook presented a comprehensive pipeline for music genre classification using the GTZAN dataset.\n",
    "Six different neural network architectures were implemented and evaluated as required by the coursework:\n",
    "\n",
    "1. Model 1: Fully connected network with two hidden layers\n",
    "2. Model 2: Convolutional neural network with custom parameters\n",
    "3. Model 3: CNN with batch normalization layers\n",
    "4. Model 4: CNN with batch normalization and RMSprop optimizer\n",
    "5. Model 5: RNN with LSTM layers and attention mechanism\n",
    "6. Model 6: RNN with LSTM layers and GAN-based data augmentation\n",
    "\n",
    "Key observations from the experiments:\n",
    "\n",
    "- Adding batch normalization (Model 3) improved performance over the basic CNN (Model 2)\n",
    "- Using the RMSprop optimizer (Model 4) showed performance differences compared to Adam\n",
    "- The LSTM-based models (5 & 6) performed well on the sequential audio data\n",
    "- GAN-based data augmentation helped improve the model's ability to generalize\n",
    "\n",
    "The best performing model was [insert best model here based on results], achieving a test accuracy of\n",
    "[insert accuracy here]%. This demonstrates the effectiveness of [insert relevant technique] for\n",
    "music genre classification tasks.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
